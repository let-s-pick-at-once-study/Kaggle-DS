{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LinearDiscriminantAnlysis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b509c3fa3027>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminant_analysis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearDiscriminantAnlysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LinearDiscriminantAnlysis'"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnlysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Initializing from file failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-92e84bc922e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#####Load train and Test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D:/학교/스터디/Data/1st_Titanic/train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D:/학교/스터디/Data/1st_Titanic/test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mIDtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"PassengerId\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8895)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Initializing from file failed"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "#####Load train and Test set\n",
    "\n",
    "train = pd.read_csv(\"D:/학교/스터디/Data/1st_Titanic/train.csv\")\n",
    "test=pd.read_csv(\"D:/학교/스터디/Data/1st_Titanic/test.csv\")\n",
    "IDtest=test[\"PassengerId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_outlieres(df, n, features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to tje Tukey method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    \n",
    "    #iterate over features(columns)\n",
    "    for col in features:\n",
    "        #1st quartile(25%)\n",
    "        Q1=np.percentile(df[col], 25)\n",
    "        #3rd quartile(75%)\n",
    "        Q3=np.percentile(df[col],75)\n",
    "        #Interquartile range(IQR)\n",
    "        IQR=Q3-Q1\n",
    "        \n",
    "        #outlier ster\n",
    "        outlier_step=1.5*IQR\n",
    "        \n",
    "        #Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col]<Q1-outlier_step) | (df[col]>Q3+outlier_step)].index\n",
    "        \n",
    "        #append the found outlier indices for col to the list of outlier indices\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "        #select observations containing more than 2 outliers\n",
    "        outlier_indices = Counter(outlier_indices)\n",
    "        multiple_outliers = list(k for k, v in outlier_indices.items() if v>n)\n",
    "        \n",
    "        return multiple_outliers\n",
    "    \n",
    "    #detect outliers from Age, SibSp, Parch and Fare\n",
    "    Outliers_do_drop=detect_outliers(train, 2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f7b5f353fc0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutliers_to_drop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train.loc[Outliers_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop outliers\n",
    "train=train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Join train and test datasets in order to obtain the same number of features during categoriacl conversion\n",
    "train_len=len(train)\n",
    "dataset=pd.concat(objs=[train, test], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fill empty and NaNs values with NaN\n",
    "dataset=dataset.fillna(np.nan)\n",
    "\n",
    "#Check for Null values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Infos\n",
    "train.info()\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Summarize data\n",
    "#Summarie and statistics\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Correlation matrix between numerical values(SibSp Parch Age and Fare values) and Survived\n",
    "g=sns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explore SibSp feature vs Survived\n",
    "g=sns.factorplot(x=\"SibSp\", y=\"Survived\", data=train, kind=\"bar\", size=6, palette=\"muted\")\n",
    "g.despind(left=True)\n",
    "g=g.set_ylabels(\"survival probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explore Parch feature vs Survived\n",
    "g=sns.factorplot(x=\"Parch\", y=\"Survived\", data=train, kind=\"bar\", size=6, palette=\"muted\")\n",
    "g.despine(left=True)\n",
    "g=g.set_ylabels(\"survival probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explore Age vs Survived\n",
    "g=sns.FacetGrid(train, col='Survived')\n",
    "g=g.map(sns.distplot, \"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explore Age distibution\n",
    "g=sns.kdeplot(train[\"Age\"][(train[\"Survived\"]==0)&(train[\"Age\"].notnull())], color=\"Red\", shade=True)\n",
    "g=sns.kdeplot(train[\"Age\"][(train[\"Survived\"]==1)&(train[\"Age\"].notnull())], ax=g, color=\"Blue\", shade=True)\n",
    "g.set_xlabel(\"Age\")\n",
    "g.set_ylabel(\"Frequency\")\n",
    "g=g.legend([\"Not Survived\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"Fare\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fill Fare missing values with the median value\n",
    "dataset[\"Fare\"]=dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explore Fare distribution\n",
    "g=sns.distplot(dataset[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\n",
    "g=g.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply log to Fare to reduce skewness distribution\n",
    "dataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i : np.log(i) if i>0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=sns.distplot(dataset[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%dataser[\"Fare\"].skew())\n",
    "g=g.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=sns.barplot(x=\"Sex\", y=\"Survived\", data=train)\n",
    "g=g.set_ylabel(\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[[\"Sex\", \"Survived\"]].groupby('Sex').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explore Pclass vs Survived\n",
    "g=sns.factorplot(x=\"Pclass\", y=\"Survived\", data=train, kind=\"bar\", size=6, palette=\"muted\")\n",
    "g.despine(left=True)\n",
    "g=g.set_ylabels(\"survival probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explore Pclass vs Survived by Sex\n",
    "g=sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train, size=6, kind=\"bar\", palette=\"muted\")\n",
    "g.despine(left=True)\n",
    "g=g.set_ylabels(\"survival probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"Embarked\"].sinull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fill Embarked nan values of dataset set with 'S' most frequent value\n",
    "dataset[\"Embarked\"]=dataset[\"embarked\"].fillna(\"S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explore Embarked vs Survived\n",
    "g=sns.factorplot(x=\"Embarked\", y=\"Survived\", data=train, size=6, kind=\"bar\", palette=\"muted\")\n",
    "g.despine(left=True)\n",
    "g=g.set_ylabels(\"survival probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explore Pclass vs Embarked\n",
    "g=sns.factorplot(\"Pclass\", col=\"Embarked\", data=train, size=6, kind=\"count\", palette=\"muted\")\n",
    "g.despine(left=True)\n",
    "g=g.set_ylabels(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explore Age vs Sex, Parch, Pclass and SibSP\n",
    "g=sns.factorplot(y=\"Age\", x=\"Sex\", data=dataset, kind=\"box\")\n",
    "g=sns.factorplot(y=\"Age\", x=\"Sex\", hue=\"Pclass\", data=dataset, kind=\"box\")\n",
    "g=sns.factorplot(y=\"Age\", x=\"Parch\", data=dataset, kind=\"box\")\n",
    "g=sns.factorplot(y=\"Age\", x=\"SibSp\", data=dataset, kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert Sex into categorical value 0 for male and 1 for female\n",
    "dataset[\"Sex\"]=dataset[\"Sex\"].map({\"male\":0, \"female\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=sns.heatmap(dataset[[\"Age\", \"Sex\", \"SibSp\", \"Parch\", \"Pclass\"]].corr(), cmap=\"BrBG\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filling missing value of Age\n",
    "\n",
    "##Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n",
    "#Index of NaN age rows\n",
    "index_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n",
    "\n",
    "for i in index_NaN_age:\n",
    "    age_med = dataset[\"Age\"].median()\n",
    "    age_pred=dataset[\"Age\"][((dataset['SibSp']==dataset.iloc[i][\"SibSp\"])&(dataset['Parch']==dataset.iloc[i][\"Parch\"]) & (dataset['Pclass']==dataset.iloc[i][\"Pclass\"]))].median()\n",
    "    if not np.isnan(age_pred):\n",
    "        dataset['Age'].iloc[i] = age_pred\n",
    "    else:\n",
    "        dataset['Age'].iloc[i] = age_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=sns.factorplot(x=\"Survived\", y=\"Age\", data=train, kind=\"box\")\n",
    "g=sns.factorplot(x=\"Survived\", y=\"Age\", data=train, kind=\"violin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"Name\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get Title from Name\n",
    "dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\n",
    "dataset[\"Title\"]=pd.Series(dataset_title)\n",
    "dataset[\"Title\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=sns.countplot(x=\"Title\", data=dataset)\n",
    "g=plt.setp(g.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert to categorical values Title\n",
    "dataset[\"Title\"]=dataset[\"Title\"].replace(['Lady', 'the Countess', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "dataset[\"Title\"]=dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\":1, \"Mme\":1, \"Mile\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n",
    "dataset[\"Title\"]=dataset[\"Title\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=sns.countplot(dataset[\"Title\"])\n",
    "g=g.set_xticklabels([\"Master\", \"Miss/Ms/Mme/Mlle/Mrs\", \"Mr\", \"Rare\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=sns.factorplot(x=\"Title\", y=\"Survived\", data=dataset, kind=\"bar\")\n",
    "g=g.set_xticklabels([\"Master\", \"Miss-Mrs\", \"Mr\", \"Rare\"])\n",
    "g=g.set_ylabels(\"survival probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop Name variable\n",
    "dataset.drop(labels = [\"Name\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a family size descriptor from SibSp and Parch\n",
    "dataset[\"Fsize\"]=dataset[\"SibSp\"]+dataset[\"Parch\"]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=sns.factorplot(x=\"Fsize\", y=\"Survived\", data=dataset)\n",
    "g=g.set_ylabels(\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create new feature of family size\n",
    "dataset['Single']=dataset['Fsize'].map(lambda s:1 if s=1 dlse 0)\n",
    "dataset['SmallF']=dataset['Fsize'].map(lambda s:1 if s==2 else 0)\n",
    "dataset['MedF']=dataset['Fsize'].map(lambda s:1 if 3<=s<=4 else 0)\n",
    "dataset['LargeF']=dataset['Fsize'].map(lambda s:1 if s>=5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=sns.factorplot(x=\"Single\", y=\"Survived\", data=dataset, kind=\"bar\")\n",
    "g=g.set_ylabels(\"Survival Provavility\")\n",
    "g=sns.factorplot(x=\"SmallF\", y=\"Survived\", data=dataset, kind=\"bar\")\n",
    "g=g.set_ylabels(\"Survival Probability\")\n",
    "g=sns.factorplot(x=\"MedF\", y=\"Survived\", data=dataset, kind=\"bar\")\n",
    "g=g.set_ylabels(\"Survival Probability\")\n",
    "g=sns.factorplot(x=\"LargeF\", y=\"Survived\", data=dataset, kind=\"bar\")\n",
    "g=g.set_ylabels(\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert to indicator values Title and Embarked\n",
    "dataset=pd.get_dummies(dataset, columns = [\"Title\"])\n",
    "dataset=pd.get_dummies(dataset, columns=[\"Emvarked\"], prefix=\"Em\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"Cabin\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"Cabin\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"Cabin\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"Cabin\"][dataset[\"Cabin\"].notnull()].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Replace the Cabin number by the type of cabin 'X' if not\n",
    "dataset[\"Cabin\"]=pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=sns.countplot(dataset[\"Cabin\"], order=['A','B','C','D','E','F','G','T','X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=sns.factorplot(y=\"Survived\", x=\"Cabin\", data=dataset, kind=\"bar\", order=['A','B','C','D','E','F','G','T','X'])\n",
    "g=g.set_ylabels(\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset=pd.get_dummies(dataset, columns = [\"Cabin\"], prefix=\"Cabin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"Ticked\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Treat Ticket by extracting the ticked prefix. When there is no prefix it returns X.\n",
    "\n",
    "Ticket=[]\n",
    "for i in list(dataset.Ticket):\n",
    "    if not i.isdigit():\n",
    "        Ticket.append(i.replace(\".\", \"\").replace(\"/\", \"\").strip().split('  ')[0])\n",
    "    else:\n",
    "        Ticket.append(\"X\")\n",
    "        \n",
    "dataset[\"Ticket\"]=Ticket\n",
    "dataset[\"Ticket\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset=pd.get_dummies(dataset, columns=[\"Ticket\"], prefix=\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create categorical values for Pclass\n",
    "dataset[\"Pclass\"]=dataset[\"Pclass\"].astype(\"category\")\n",
    "dataset=pd.get_dummies(dataset, columns=[\"Pclass\"], prefix=\"Pc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop useless variables\n",
    "dataset.drop(labels=[\"PassengerId\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Separate train dataset and test dataset\n",
    "\n",
    "train = dataset[:train_len]\n",
    "test=dataset[train_len:]\n",
    "test.drop(labels=[\"Survived\"], axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Separate train features and label\n",
    "\n",
    "train[\"Survived\"]=train[\"Survived\"].astype(int)\n",
    "\n",
    "Y_train=train[\"Survived\"]\n",
    "\n",
    "X_train=train.drop(labels=[\"Survived\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross validate model with Kfold stratified cross val\n",
    "kfold=StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Modeling step Test differents algorithms\n",
    "random_state=2\n",
    "classifiers = []\n",
    "classifiers.append(SVC(random_state=random_state))\n",
    "classifiers.append(DecisionTreeClassifier(random_state=random_state))\n",
    "classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state), random_state=random_state, learning_rate=0.1))\n",
    "classifiers.append(RandomForestClassifier(random_state=random_state))\n",
    "classifiers.append(ExtraTreesClassifier(random_state=random_state))\n",
    "classifiers.append(GradientBoostingClassifier(random_state=random_state))\n",
    "classifiers.append(MLPClassifier(random_state=random_state))\n",
    "classifiers.append(KNeighborsClassifier())\n",
    "classifiers.append(LogisticRegression(random_state=random_state))\n",
    "classifiers.append(LinearDiscriminantAnalysis())\n",
    "\n",
    "cy_results=[]\n",
    "for classifier in classifiers:\n",
    "    cy_results.append(cross_val_score(classifier, X_train, y=Y_train, scoring=\"accuracy\", cv=kfold, n_jobs=4))\n",
    "    \n",
    "cv_means=[]\n",
    "cv_std=[]\n",
    "for cy_result in cy_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "    \n",
    "cv_res=pe.DataFrame({\"CrossValMeans\":cv_means, \"CrossValerrors\":cv_Std,\n",
    "                    \"Algorithm\":[\"SVC\",\"DecisionTree\", \"AdaBoost\",\"RandomForest\",\n",
    "                                \"ExtraTrees\", \"GradientBoosting\",\"MultipleLayerPerceptron\",\n",
    "                                \"KNeighboors\", \"LogisticRegression\", \"LinearDiscriminantAnalysis\"]})\n",
    "\n",
    "g=sns.barplot(\"CrossValMeans\", \"Algorithm\", data=cv_res, palette=\"Set3\", orient=\"h\", **{'xerr':cv_std})\n",
    "g.set_xlabel(\"Mean Accuracy\")\n",
    "g=g.set_title(\"Cross validation scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###META MODELING WITH ADABOOST, RF, EXTRATRESS and GRADIENT BOOSTING\n",
    "\n",
    "#Adaboost\n",
    "DTC=DecisionTreeClassifier()\n",
    "\n",
    "adaDTC=adaBoostClassifier(DTC, random_state=7)\n",
    "\n",
    "ada_param_grid={\"base_estimator__criterion\":[\"gini\", \"entropy\"],\n",
    "               \"base_estimator_splitter\":[\"best\", \"random\"],\n",
    "               \"algorithm\" : [\"SAMME\", \"SAMME.R\"],\n",
    "               \"n_estimators\":[1,2],\n",
    "               \"learning_rate\":[0.0001, 0.001. 0.01, 0.1, 0.2, 0.3, 1.5]}\n",
    "\n",
    "gsadaDTC=GridSearchCV(adaDTC, param_grid=ada_parm_grid, cv=kfold, scoring=\"accuracy\", n_jobs=4, verbose=1)\n",
    "\n",
    "gsadaDTC.fit(X_train, Y_train)\n",
    "\n",
    "ada_best=gsadaDTC.best_bstimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsadaDTC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ExtraTrees\n",
    "ExcC=ExtraTreesClassifier()\n",
    "\n",
    "##Search grid for optimal parameters\n",
    "ex_param_grid={\"max_depth\":[Nome],\n",
    "              \"max_features\":[1,3,10],\n",
    "              \"min_samples_split\":[2,3,10],\n",
    "              \"min_samples_leaf\":[1,3,10],\n",
    "              \"bootstrap\":[False],\n",
    "              \"n_estimators\":[100,300],\n",
    "              \"criterion\":[\"gini\"]}\n",
    "\n",
    "gsExtC=GridSearchCV(ExtC, param_grid=ex_param_grid, cv=kfold,\n",
    "                   scoring=\"accuracy\", n_jobs=4, verbose=1)\n",
    "\n",
    "gsExtC.fit(X_train, Y_train)\n",
    "\n",
    "ExtC_best=gsExtC.best_estimator_\n",
    "\n",
    "#Best score\n",
    "gsExtC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RFC Parameters tunning\n",
    "RFC=RandomForestClassifier()\n",
    "\n",
    "##Search grid for optimal parameters\n",
    "fr_param_grid={\"max_Depth\":[None],\n",
    "              \"max_Features\":[1,3,10],\n",
    "              \"min_samples_leaf\":[1,3,10],\n",
    "              \"bootstrap\":[False],\n",
    "              \"n_estimators\":[100,300],\n",
    "              \"criterion\":[\"gini\"]}\n",
    "\n",
    "gsRFC=GridSearchCV(RFC, param_grid=rf_param_grid, cv=kfold, scoring=\"accurach\",\n",
    "                  n_jobs=4, verbose=1)\n",
    "\n",
    "gsRFC.fit(X_train, Y_train)\n",
    "\n",
    "RFC_best=gsRFC.best_estimator_\n",
    "\n",
    "#Best score\n",
    "gsRFC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gradient boosting tunning\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "gb_param_grid={'loss':[\"deviance\"],\n",
    "              'n_estimators':[100,200,300],\n",
    "              'learning_rate':[0.1,0.05,0.01],\n",
    "              'max_depth':[4,8],\n",
    "              'min_samples_leaf':[100,150],\n",
    "              'max_features':[0.3,0.1]}\n",
    "\n",
    "gsGBC=GridSearchCV(GBC, param_grid=gb_param_grid, cv=kfold,\n",
    "                  scoring=\"accuracy\",n_jobs=4, verbose=1)\n",
    "\n",
    "gsGBC.fit(X_train, Y_train)\n",
    "\n",
    "GBC_best=gsGBC.best_estimator\n",
    "\n",
    "#Best score\n",
    "gsGBC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###SVC classifier\n",
    "SVMC=SVC(probability=True)\n",
    "svc_param_grid={'kernel':['rbf'],\n",
    "               'gamma':[0.001, 0.01, 0.1, 1],\n",
    "               'C':[1,10,50,100,200,300,1000]}\n",
    "\n",
    "gsSVMC=GridSearchCV(SVMC, param_grid=svc_param_Grid, cv=kfold,\n",
    "                   scoring=\"accuracy\", n_jobs=4, verbose=1)\n",
    "\n",
    "gsSVMC.fit(X_train, Y_train)\n",
    "\n",
    "SVMC_best=gsSVMC.best_estimator_\n",
    "\n",
    "#Best score\n",
    "gsSVMC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, nlim=None, cv=None,\n",
    "                       n_jobs=-1, train_sizes=np.linspace(0.1, 1.0,5)):\n",
    "    \"\"\"Generate a simple plot of the test and training liarning curve\"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylin(*ylin)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_Scores, test_Scores=learning_curve(estimator, X, y, cv=cv, \n",
    "                                                         n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean=np.mean(train_scores, axis=1)\n",
    "    train_scores_std=np.std(train_scores, axis=1)\n",
    "    test_scores_mean=np.mean(test_scores, axis=1)\n",
    "    test_scores_std=np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                    test_scores_mean + test_Scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "            label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "g=plot_learning_curve(gsRFC.best_, \"RF mearning curves\", X_train, Y_train, cv=kfold)\n",
    "g=plot_learning_curve(gsExtC.best_estimator_, \"ExtraTrees learning curves\",\n",
    "                     X_train, Y_train, cv=kfold)\n",
    "g=plot_learning_curve(gsSVMC.best_estimator_, \"SVC learning curves\",\n",
    "                     X_train, Y_train, cv=kfold)\n",
    "g=plot_learning_curve(gsadaDTC.best_estimator_, \"AdaBoost learning curves\",\n",
    "                     X_train, Y_train, cv=kfold)\n",
    "g=plot_learning_curve(gsGBC.best_estimator_, \"GradientBoosting learning curves\",\n",
    "                     X_train, Y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrows=ncols=2\n",
    "fig, axes=plt.subplots(nrows=nrows, ncols=ncols, sharex=\"all\", figsize(15,15))\n",
    "\n",
    "names_classifiers=[(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ExtC_best),\n",
    "                  (\"RandomForest\",RFC_best),(\"GradientBoosting\", GBC_best)]\n",
    "\n",
    "nclassifier=0\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "        name=names_classifiers[nclassifier][0]\n",
    "        classifier=names_classfiers[nclassifier][1]\n",
    "        indices=np.argsort(classfier.feature_importances_)[::-1][:40]\n",
    "        g=sns.barplot(y=X_train.columns[indices][:40],\n",
    "                     x=classifier.feature_importances_[indices][:40],\n",
    "                     orient='h', ax=axes[row][col])\n",
    "        g.set_xlabel(\"Relative importance\", fontsize=12)\n",
    "        g.set_ylabel(\"Features\", fontsize=12)\n",
    "        g.tick_params(labelsize=9)\n",
    "        g.set_title(name + \"feature importance\")\n",
    "        nclassifier += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_Survived_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\n",
    "test_Survived_ExtC=pd.Series(ExtC_best.predict(test), name=\"ExtC\")\n",
    "test_Survived_SVMC=pd.Series(SVMC_best.predict(test), name=\"SVC\")\n",
    "test_Survived_AdaC=pd.Series(ada_best.predict(test), name=\"Ada\")\n",
    "test_Survived_GBC=pd.Series(GBC_best.predict(test), name=\"GBC\")\n",
    "\n",
    "#Concatenate all classifier results\n",
    "ensemble_results=pd.concat([test_Survived_RFC, test_Survived_ExtC,\n",
    "                           test_Survived_AdaC, test_Survived_GBC,\n",
    "                           test_Survived_SVMC], axis=1)\n",
    "\n",
    "g=sns.geatmap(ensemble_results.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_Survived=pd.Series(votingC.predict(test), name=\"Survived\")\n",
    "\n",
    "results=pd.concat([IDtest, test_Survived], axis=1)\n",
    "\n",
    "results.to_csv(\"ensemble_python_voting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "votingC=VotingClassfier(estimators[('rfc', RFC_best),\n",
    "                                  ('extc', ExtC_best),\n",
    "                                  ('svc', SVMC_best),\n",
    "                                  ('adac', ada_best),\n",
    "                                  ('gbc', gbc_best)],\n",
    "                       voting='soft', n_jobs=4)\n",
    "\n",
    "votingC=votingC.fit(X_train, Y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
